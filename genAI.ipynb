{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototype\n",
    "genAI - an autonomous system that take a Common Core learning standard and a topic of student interest and generate customized free-response questions that test the studentâ€™s knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Initialization -----\n",
    "import pandas as pd\n",
    "import openai\n",
    "import random\n",
    "\n",
    "# Constants and Configuration\n",
    "API_KEY = 'YOUR_OPENAI_API_KEY'  # Replace with an OpenAI secret API\n",
    "MODEL_NAME = \"gpt-3.5-turbo\" \n",
    "\n",
    "# Set up the OpenAI API key\n",
    "openai.api_key = API_KEY\n",
    "\n",
    "# ----- Functions -----\n",
    "def load_ela_ccss(file_path='ela-ccss.csv'):\n",
    "    \"\"\"\n",
    "    Load the English Language Arts (ELA) Common Core State Standards (CCSS) into a dataframe, \n",
    "    filtering only writing standards for grades 4 and above.\n",
    "    Database source: https://gist.github.com/philngo/2735248c98c3e0cd7814\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path (str): Path to the CSV file containing the standards. Default is 'ela-ccss.csv'.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame: A dataframe containing all the ELA CCSS standards.\n",
    "    \"\"\"\n",
    "    ela_ccss = pd.read_csv(file_path)\n",
    "    ela_ccss = ela_ccss[ela_ccss['category_id'] == 'W']  # Only writing standards\n",
    "    ela_ccss['grade_num'] = ela_ccss['grade_id'].str.extract(r'^(\\d+)').astype(float)  # Extract grade number\n",
    "    ela_ccss = ela_ccss[ela_ccss['grade_num'] >= 4]  # Only grades 4 and above\n",
    "    return ela_ccss\n",
    "\n",
    "def chat_with_openai(prompt, max_tokens=150, temperature=0.2, conversation_history=None):\n",
    "    \"\"\"\n",
    "    Interact with OpenAI API given a prompt and return the response content.\n",
    "    \n",
    "    Parameters:\n",
    "    - prompt (str): The prompt to send to the OpenAI API.\n",
    "    - max_tokens (int): Maximum number of tokens for the response. Default is 150.\n",
    "    - temperature (float): Sampling temperature for the response. Default is 0.2.\n",
    "    - conversation_history (list): Previous conversation history. Default is None.\n",
    "    \n",
    "    Returns:\n",
    "    - tuple: A tuple containing the assistant's response and the updated conversation history.\n",
    "    \"\"\"\n",
    "    system_message = \"\"\"\n",
    "    You are a helpful assistant specializing in assessing abilities as per The Common Core State Standards (CCSS)\n",
    "    for English Language Arts (ELA). Keep track of the conversation to provide relevant outputs.\n",
    "    Do not write explanations. Do not reveal what is being assessed.\n",
    "    \"\"\".strip()\n",
    "\n",
    "    # Initialize conversation history if not provided\n",
    "    if not conversation_history:\n",
    "        conversation_history = [{\"role\": \"system\", \"content\": system_message}]\n",
    "    \n",
    "    # Add the user's prompt to the conversation history\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "    # Send the prompt to the OpenAI API\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=conversation_history,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    \n",
    "    # Get the assistant's response and add it to the conversation history\n",
    "    assistant_message = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
    "    \n",
    "    return assistant_message.strip(), conversation_history\n",
    "\n",
    "def get_standard(code, ela_ccss=None):\n",
    "    \"\"\"\n",
    "    Get the description and corresponding grade of a specific Common Core State Standard (CCSS) code for ELA.\n",
    "    \n",
    "    Parameters:\n",
    "    - code (str): The CCSS code (e.g. 'CCSS.ELA-LITERACY.W.4.9').\n",
    "    - ela_ccss (DataFrame, optional): The ELA CCSS dataframe. If not provided, default will be loaded.\n",
    "    \n",
    "    Returns:\n",
    "    - tuple: A tuple containing the description and grade of the CCSS code.\n",
    "    \"\"\"\n",
    "    if ela_ccss is None:\n",
    "        ela_ccss = load_ela_ccss()\n",
    "    description = ela_ccss[ela_ccss['id'] == code]['description'].iloc[0]\n",
    "    grade = ela_ccss[ela_ccss['id'] == code]['grade_id'].iloc[0]\n",
    "    return description, grade\n",
    "\n",
    "def generate_question(learning_standard, interest_topic, conversation_history=None, ela_ccss=None):\n",
    "    \"\"\"\n",
    "    Generates a question based on the learning standard and interest topic.\n",
    "\n",
    "    Parameters:\n",
    "    - learning_standard (str): The Common Core State Standard (CCSS) code (e.g. 'CCSS.ELA-LITERACY.W.4.9').\n",
    "    - interest_topic (str): The topic of interest for the question.\n",
    "    - conversation_history (list): Previous conversation history. Default is None.\n",
    "    - ela_ccss (DataFrame, optional): The ELA CCSS dataframe. If not provided, default will be loaded.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing the generated question and the updated conversation history.\n",
    "    \"\"\"\n",
    "    ccss_description, grade = get_standard(learning_standard, ela_ccss=ela_ccss)\n",
    "    prompt = f\"\"\"\n",
    "    Create a free-response question for a student of grade {grade} that prompts an answerer that '{ccss_description}'.\n",
    "    Additionally, provide an introduction and context required to answer the question,\n",
    "    and make it based on the topic of '{interest_topic}'. \n",
    "    In cases where the learner is expected to interpret a text, provide the text as well.\n",
    "    Do not require the learner to have any prior knowledge or access to external resources, unless\n",
    "    the learning standard requires them to do so.\n",
    "    You need to provide the learner with all the information required to answer the question.\n",
    "    The introduction and context should be around 200 words.\n",
    "    \"\"\"\n",
    "    question, conversation_history = chat_with_openai(prompt, 600, conversation_history=conversation_history)\n",
    "    return question, conversation_history\n",
    "\n",
    "def generate_rubric(learning_standard, conversation_history, ela_ccss=None):\n",
    "    \"\"\"\n",
    "    Generates a rubric based on the question.\n",
    "    \n",
    "    Parameters:\n",
    "    - learning_standard (str): The Common Core State Standard (CCSS) code (e.g. 'CCSS.ELA-LITERACY.W.4.9').\n",
    "    - conversation_history (list): Previous conversation history. Default is None.\n",
    "    - ela_ccss (DataFrame, optional): The ELA CCSS dataframe. If not provided, default will be loaded.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing the generated rubric and the updated conversation history.\n",
    "    \"\"\"\n",
    "    ccss_description, grade = get_standard(learning_standard, ela_ccss=ela_ccss)\n",
    "    example_rubric = \"\"\"\n",
    "    Criteria        Level 4        Level 3       Level 2       Level 1\n",
    "    ---------------------------------------------------------\n",
    "    [Criterion]    [Description]  [Description] [Description] [Description]\n",
    "    Points         4              3             2             1\n",
    "    ---------------------------------------------------------\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Based on the content and themes of the question and context you wrote, \n",
    "    devise a rubric that evaluates if the answer meets the learning standard of\n",
    "    '{ccss_description}' for a student of grade {grade}.\n",
    "    Your output should follow this format:\n",
    "    {example_rubric}\"\"\"\n",
    "    rubric, conversation_history = chat_with_openai(prompt, 700, conversation_history=conversation_history)\n",
    "    return rubric, conversation_history\n",
    "\n",
    "def evaluate_answer(answer, rubric, question):\n",
    "    \"\"\"\n",
    "    Evaluates an answer based on the rubric.\n",
    "\n",
    "    Parameters:\n",
    "    - answer (str): The answer to evaluate.\n",
    "    - rubric (str): The rubric to use for evaluation.\n",
    "    - question (str): The question that was asked.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing the evaluation and the updated conversation history.    \n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Rubric: \\n{rubric}\\n\n",
    "    Question: \\n'{question}'\\n\n",
    "    Answer: '{answer}'\\n\n",
    "    Given the answer to the question and using the rubric above. First provide the overall score of the answer.\n",
    "    Then, break down the feedback criterion-wise, and conclude with suggestions for improvement.\n",
    "    Do not mention the rubric or the specific criterions and write as if you were speaking directly to the answerer.\"\"\"\n",
    "    \n",
    "    conversation_history = [{\"role\": \"system\", \"content\": \"You are a helpful assistant. Keep track of the conversation to provide relevant outputs. Do not write explanations.\"}]\n",
    "\n",
    "    evaluation, conversation_history = chat_with_openai(prompt, 500, conversation_history=conversation_history)\n",
    "    return evaluation, conversation_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def simulate_response_to_question(question, rubric, level):\n",
    "    \"\"\"\n",
    "    Simulate a student's response to a given question at a specific proficiency level using OpenAI.\n",
    "    \n",
    "    Parameters:\n",
    "    - question (str): The assessment question.\n",
    "    - rubric (str): The rubric for the question.\n",
    "    - level (int): The proficiency level (e.g., 1, 2, 3, 4).\n",
    "    \n",
    "    Returns:\n",
    "    - tuple: A tuple containing the simulated student response and the conversation history.\n",
    "    \"\"\"\n",
    "    # Initialize conversation history\n",
    "    conversation_history = [{\"role\": \"system\", \"content\": \"You are a helpful assistant. Keep track of the conversation to provide relevant outputs. Do not write explanations.\"}]\n",
    "    prompt = f\"\"\"Rubric: \\n'{rubric}'\n",
    "\n",
    "    Analyze the provided rubric and write a free-response answer of level '{level}' to the following question:\n",
    "    '{question}'\n",
    "    \"\"\"\n",
    "    # Send the prompt to the OpenAI API    \n",
    "    answer, conversation_history = chat_with_openai(prompt, 250, conversation_history=conversation_history)\n",
    "\n",
    "    return answer, conversation_history    \n",
    "\n",
    "def run_simulation(topics):\n",
    "    \"\"\"\n",
    "    Run a simulation of the entire workflow: \n",
    "    - Generate a question based on a randomly selected learning standard.\n",
    "    - Generate a rubric for the question.\n",
    "    - Simulate a student's response at a random proficiency level.\n",
    "    - Evaluate the student's response.\n",
    "\n",
    "    Parameters:\n",
    "    - topics (list): A list of topics to use for the question.\n",
    "    \n",
    "    Prints the learning standard, question, rubric, student's response, and evaluation.\n",
    "    \"\"\"\n",
    "    # Load the ELA CCSS.\n",
    "    ela_ccss = load_ela_ccss()\n",
    "\n",
    "    # Randomly select a learning standard.\n",
    "    random_standard = random.choice(ela_ccss['id'].tolist())\n",
    "\n",
    "    # Define a topic of interest.\n",
    "    interest_topic = random.choice(topics)\n",
    "    ccss_description, grade = get_standard(random_standard)\n",
    "\n",
    "    print(f\"Learning Standard: {random_standard} - '{ccss_description}' (Grade {grade})'\")\n",
    "    print(f\"Topic of Interest: {interest_topic}\\n\")\n",
    "\n",
    "    # Generate a question based on the standard and topic.\n",
    "    generated_question, conversation_history = generate_question(random_standard, interest_topic)\n",
    "    \n",
    "    print(\"Generated Question:\", generated_question)\n",
    "\n",
    "    # Generate a rubric based on the question.\n",
    "    generated_rubric, _ = generate_rubric(random_standard, conversation_history)\n",
    "    \n",
    "    print(\"Generated Rubric:\", generated_rubric)\n",
    "\n",
    "    # Simulate a student's response at a random level.\n",
    "    answer_level = random.choice([1, 2, 3, 4])\n",
    "    student_answer, _ = simulate_response_to_question(generated_question, generated_rubric, answer_level)\n",
    "\n",
    "    print(f\"\\nStudent's Answer (level {str(answer_level)}): {student_answer}\")\n",
    "\n",
    "    # Evaluate the answer.\n",
    "    evaluation, _ = evaluate_answer(student_answer, generated_rubric, generated_question)\n",
    "\n",
    "    print(\"\\nEvaluation:\", evaluation)\n",
    "\n",
    "    return random_standard, interest_topic, generated_question, generated_rubric, answer_level, evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ['baseball', 'pop music', 'climate change', 'ancient civilizations', 'space exploration', 'wildlife conservation', 'sports']\n",
    "\n",
    "# run multiple simulations to compare results\n",
    "results_df = pd.DataFrame(columns=['model', 'standard', 'topic', 'question', 'rubric', 'answer level', 'evaluation'])\n",
    "results = []\n",
    "\n",
    "for i in range(20):\n",
    "    print(f\"Simulation {i+1}\")\n",
    "    random_standard, interest_topic, generated_question, generated_rubric, answer_level, evaluation = run_simulation(topics)\n",
    "    results.append((MODEL_NAME, random_standard, interest_topic, generated_question, generated_rubric, answer_level, evaluation))\n",
    "\n",
    "# add results to dataframe\n",
    "results_df = pd.DataFrame(results, columns=['model', 'standard', 'topic', 'question', 'rubric', 'answer level', 'evaluation'])\n",
    "\n",
    "# save results to csv\n",
    "results_df.to_csv('questions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality Control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grade(s: str) -> str:\n",
    "    parts = s.split(\".\")\n",
    "    if len(parts) >= 4:\n",
    "        return parts[3]\n",
    "    return None\n",
    "\n",
    "def quality_control(learning_standard, question, rubric):\n",
    "    \"\"\"\n",
    "    Implement a Quality Control (QC) check on the generated question, rubric & evaluation.\n",
    "\n",
    "    Parameters:\n",
    "    - learning_standard (str): The CCSS standard.\n",
    "    - grade (str): The grade level for which the question was developed.\n",
    "    - question (str): The question generated by the Model.\n",
    "    - rubric (str): The Rubric generated by the Model.\n",
    "    - evaluation(str): The answer evaluation generated by the Model.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary with keys 'question_quality', 'rubric_quality', 'evaluation_quality', \n",
    "    each value representing a rating in the scale of 1-10.\n",
    "    \"\"\"\n",
    "    grade = get_grade(learning_standard)\n",
    "    qc_results = {}\n",
    "\n",
    "    system_message = \"\"\"You are a friendly assistant that rates the quality of a text based on given instructions.\n",
    "    You have to decide whether it fits the requirements mentioned in the prompt.\n",
    "    Your rating should be between 1 and 10 and nothing else.\n",
    "    Do not write explanations. Do not reveal what is being assessed.\"\"\"\n",
    "    conversation_history = [{\"role\": \"system\", \"content\": system_message}]\n",
    "    \n",
    "    # Quality check for question\n",
    "    qc_prompt = f\"\"\"Assess the following question based on relevance to grade {grade}, \n",
    "    adherence to learning standard '{learning_standard}', and challenge level.\n",
    "    Question: \n",
    "    '{question}'\"\"\"\n",
    "    qc_results['question_quality'], conversation_history = chat_with_openai(qc_prompt, 500, conversation_history=conversation_history)\n",
    "\n",
    "    # Quality check for rubric\n",
    "    qc_prompt = f\"\"\"Assess the following rubric based on alignment to the learning standard and ability to thoroughly evaluate a student's mastery of the standard.\n",
    "    Rubric:\n",
    "    '{rubric}'\"\"\"\n",
    "    qc_results['rubric_quality'], conversation_history = chat_with_openai(qc_prompt, 500, conversation_history=conversation_history)\n",
    "\n",
    "    return qc_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>standard</th>\n",
       "      <th>topic</th>\n",
       "      <th>question</th>\n",
       "      <th>rubric</th>\n",
       "      <th>answer level</th>\n",
       "      <th>evaluation</th>\n",
       "      <th>question_quality</th>\n",
       "      <th>rubric_quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>CCSS.ELA-LITERACY.W.7.1.d</td>\n",
       "      <td>climate change</td>\n",
       "      <td>Introduction and Context:\\n\\nClimate change is...</td>\n",
       "      <td>Criteria        Level 4        Level 3       L...</td>\n",
       "      <td>3</td>\n",
       "      <td>Overall Score: Level 4\\n\\nFeedback:\\n- Clarity...</td>\n",
       "      <td>8.5</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>CCSS.ELA-LITERACY.W.4.10</td>\n",
       "      <td>climate change</td>\n",
       "      <td>Introduction and Context:\\n\\nClimate change is...</td>\n",
       "      <td>Criteria        Level 4        Level 3       L...</td>\n",
       "      <td>1</td>\n",
       "      <td>Overall Score: Level 1\\n\\nFeedback:\\n- Organiz...</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>CCSS.ELA-LITERACY.W.5.1.d</td>\n",
       "      <td>wildlife conservation</td>\n",
       "      <td>Introduction and Context:\\n\\nWildlife conserva...</td>\n",
       "      <td>Criteria        Level 4        Level 3       L...</td>\n",
       "      <td>3</td>\n",
       "      <td>Overall Score: Level 4 (4 points)\\n\\nFeedback:...</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>CCSS.ELA-LITERACY.W.8.3</td>\n",
       "      <td>pop music</td>\n",
       "      <td>Introduction and Context:\\n\\nPop music is a ge...</td>\n",
       "      <td>Criteria        Level 4        Level 3       L...</td>\n",
       "      <td>1</td>\n",
       "      <td>Overall Score: Level 1\\n\\nFeedback:\\n- The nar...</td>\n",
       "      <td>9.5</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>CCSS.ELA-LITERACY.W.5.2.b</td>\n",
       "      <td>space exploration</td>\n",
       "      <td>Introduction and Context:\\n\\nSpace exploration...</td>\n",
       "      <td>Criteria        Level 4        Level 3       L...</td>\n",
       "      <td>2</td>\n",
       "      <td>Overall Score: Level 4\\n\\nFeedback:\\n- Accurac...</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model                   standard                  topic  \\\n",
       "0  gpt-3.5-turbo  CCSS.ELA-LITERACY.W.7.1.d         climate change   \n",
       "1  gpt-3.5-turbo   CCSS.ELA-LITERACY.W.4.10         climate change   \n",
       "2  gpt-3.5-turbo  CCSS.ELA-LITERACY.W.5.1.d  wildlife conservation   \n",
       "3  gpt-3.5-turbo    CCSS.ELA-LITERACY.W.8.3              pop music   \n",
       "4  gpt-3.5-turbo  CCSS.ELA-LITERACY.W.5.2.b      space exploration   \n",
       "\n",
       "                                            question  \\\n",
       "0  Introduction and Context:\\n\\nClimate change is...   \n",
       "1  Introduction and Context:\\n\\nClimate change is...   \n",
       "2  Introduction and Context:\\n\\nWildlife conserva...   \n",
       "3  Introduction and Context:\\n\\nPop music is a ge...   \n",
       "4  Introduction and Context:\\n\\nSpace exploration...   \n",
       "\n",
       "                                              rubric  answer level  \\\n",
       "0  Criteria        Level 4        Level 3       L...             3   \n",
       "1  Criteria        Level 4        Level 3       L...             1   \n",
       "2  Criteria        Level 4        Level 3       L...             3   \n",
       "3  Criteria        Level 4        Level 3       L...             1   \n",
       "4  Criteria        Level 4        Level 3       L...             2   \n",
       "\n",
       "                                          evaluation question_quality  \\\n",
       "0  Overall Score: Level 4\\n\\nFeedback:\\n- Clarity...              8.5   \n",
       "1  Overall Score: Level 1\\n\\nFeedback:\\n- Organiz...                9   \n",
       "2  Overall Score: Level 4 (4 points)\\n\\nFeedback:...                7   \n",
       "3  Overall Score: Level 1\\n\\nFeedback:\\n- The nar...              9.5   \n",
       "4  Overall Score: Level 4\\n\\nFeedback:\\n- Accurac...                8   \n",
       "\n",
       "  rubric_quality  \n",
       "0            9.5  \n",
       "1              8  \n",
       "2              8  \n",
       "3            8.5  \n",
       "4              9  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run quality control on the generated results and add results as new columns\n",
    "qc_results = []\n",
    "for i, row in results_df.iterrows():\n",
    "    qc_results.append(quality_control(row['standard'], row['question'], row['rubric']))\n",
    "    # add results to dataframe\n",
    "    results_df.loc[i, 'question_quality'] = qc_results[i]['question_quality']\n",
    "    results_df.loc[i, 'rubric_quality'] = qc_results[i]['rubric_quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the evaluation quality, we will determine it by comparing the evaluation score \n",
    "# to the answer level (which is the input provided to the model to generate the answer in the simulation).\n",
    "\n",
    "# Extract overall score using regex and add to a new column\n",
    "results_df['eval_score'] = results_df['evaluation'].str.extract(r\"Overall Score: Level (\\d+)\")[0]\n",
    "\n",
    "# Convert numeric columns\n",
    "results_df['answer level'] = pd.to_numeric(results_df['answer level'], errors='coerce')\n",
    "results_df['question_quality'] = pd.to_numeric(results_df['question_quality'], errors='coerce')\n",
    "results_df['rubric_quality'] = pd.to_numeric(results_df['rubric_quality'], errors='coerce')\n",
    "results_df['eval_score'] = pd.to_numeric(results_df['eval_score'], errors='coerce')\n",
    "\n",
    "# Calculate evaluation quality\n",
    "results_df['evaluation_quality'] = 10 * (1 - abs(results_df['eval_score'] - results_df['answer level']) / results_df[['eval_score', 'answer level']].max(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to a CSV file\n",
    "results_df.to_csv('qc-results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
